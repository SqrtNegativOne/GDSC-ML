{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# honestly this is seriously getting out of hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM FORESTS ARE SO FUCKING SLOW min_weight_fraction_leaf\n",
    "AND WHY TF DID IT REPLACE WTF WITH min_weight_fraction_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 100, log = True)\n",
    "    if max_depth == 100:\n",
    "        max_depth = None\n",
    "    \n",
    "    boostrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 2000),\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 128, log=True),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 64, log=True),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': boostrap,\n",
    "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0, 0.5),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 1000),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0, 1),\n",
    "        'oob_score': trial.suggest_categorical('oob_score', [True, False]) if boostrap else False,\n",
    "        'ccp_alpha': trial.suggest_float('ccp_alpha', 0, 1),\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.1, 1.0) if boostrap else None,\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    \n",
    "    scores = cross_val_score(rf, X_train, y_train,\n",
    "                             cv = 5,\n",
    "                             scoring = 'accuracy')\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "print('Best score: ', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from optuna.integration import LightGBMPruningCallback\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "        'verbosity': -1 # oh my god shut up\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_error\",\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, \"binary_error\")\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        preds = model.predict(X_test)  # Changed this line\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)  # Using accuracy on predicted labels\n",
    "\n",
    "\n",
    "    print(cv_scores)\n",
    "    return 1 - cv_scores[-1]\n",
    "\n",
    "study = optuna.create_study(study_name = 'START AGAIN', direction = 'minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "print('Best score: ', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DS = Dataset(X_train, label = y_train)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n",
    "        'eval_metric': 'error',\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            early_stopping_rounds=100,\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = accuracy_score(y_test, preds)\n",
    "\n",
    "    return cv_scores[-1]\n",
    "\n",
    "study = optuna.create_study(study_name = 'START AGAIN', direction = 'maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "print('Best score: ', study.best_value)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
